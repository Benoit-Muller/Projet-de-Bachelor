\chapter{Linear Ordinary Equations}
\section{Solutions Space}
In this chapter,we study explicit autonomous ordinary equations of first order that are linear, namely equations of the form 
\[\dotxx=A\xx\]
where $A$ is a $n\times n$ matrix and $\xx$ is a differentiable function from a real interval to $\R^n$. Note that as its derivative must be $C^\infty$, $\xx$ must be $C^\infty$ to exist. 

First of all, we observe that the set of solution in a fixed interval is a vector subspace of $C^\infty$. Indeed for all solutions $\xx$, $\yy$ and a scalar $\a$, \[(\a\xx+\yy)'=\a\dotxx + \dot{\yy}=\a A\xx + A\yy=A(\a\xx+\yy)\] and the identical null function is trivially in the space. This motivate us to find a basis of this subspace and understand how to construct it.
\begin{definition}
    A collection of $k$ solutions $\xx_1,\dots,\xx_k$ on $I$ are said to be \emph{linearly independents} or \emph{independents}, if $\xx_1(t),\dots,\xx_k(t)$ are linearly independents for all $t\in I$.
\end{definition}
\begin{lemme}
A collection $\xx_1,\dots,\xx_k$ on $I$ of solutions are linearly independents if and only if there exists a moment when their positions are linearly independents.
\end{lemme}
\begin{proof}
If the solutions are independent we have the result by definition. In the other way, as $\xx\mapsto A\xx$ is Lipschitz continuous, we have by Picard that the solutions are uniques for a initial point given. Now if at a certain time $\tau$, the positions aren't independent, then we must have non all null scalar $\a_1,\dots,\a_k$ such that  $\a_1\xx_1(\tau) + \dots + \a_k\xx_k(\tau)=0$ but then 0 is a solution starting there and then actually  $\a_1\xx_1(t) + \dots + \a_k\xx_k(t)=0$ for all $t$ in $I$ and .
\end{proof}
This lemma prove that the space of solution has the same dimension the space $\R^n$ i.e. $n$.
In order to manipulate the solutions, we put them in a matrix, like $(\xx_1,\dots,\xx_k)$. In this form we see that we can actually extend the equation to matrix entries: 
\[\dot{X}=AX\]
and since 
\[\dot{X}=(\dotxx_1,\dots,\dotxx_k)  
\text{ and } 
AX=A(\xx_1,\dots,A\xx_k)=(A\xx_1,\dots,A\xx_k)\]
A matrix X is a solution if and only if its columns are vector solutions. We see that for a vector $v\in\R^k$, $Xv$ is a vector solution $(Xv)' = \dot{X}v = AXv$. This solution ca be written as $Xv = v_1\xx_1 + \dots + v_k\xx_k$ and is thus a linear combination of the column solutions of $X$. Thus a matrix solution permit us to easily write new solutions as linear combinaison of a collection of solutions. As the dimension of the space of solutions is $n$,exactly $n$ linear independents solutions will be enough to construct all solutions as a product of the matrix solution constructed with them.
\begin{definition}
    If a matrix solution $M=(\xx_1,\dots,A\xx_n)$ is square(k=n) and of full rank, then it is called a emph{fundamental matrix solution} and $\{\xx_1,\dots,A\xx_k\}$ a \emph{fundamental set of solutions}.
\end{definition}
with such a $M$, $M\R^n$ is the all space of solutions. At a time $t=0$, $M(0)v=v_1\xx_1(0) + \dots + v_k\xx_k(0)$ meaning that with the condition $\xx(0)=x_0$, v must be chosen to be the vector of coordinates of $x_0$ in the basis of the columns of M.
\\ \\
This done, we can look at the forms the solutions can take. In one dimension, the problem is $\dotx=ax$ and if a is non null, non trivial solutions respect $a=(\log|x|)'$ which give $\log|x(t)|=at+c$ and $x(t)=\pm e^{at+c}=x_0e^{at}$. We check easily that $e^{at}$ is indeed a solution. The $n$-dimensional case is more complex. We try to put the equation in its integral form and derive a property by recurrence (supposing $X_0=I$ for simplicity): 
\begin{IEEEeqnarray*}{rCl}
X(t) 
&=& I + \int_0^t AX(s) \dd s 
= I + \int_0^t A\bigg(I + \int_0^{s_1} AX(s_2) \dd s_2\bigg) \dd s_1
= I + tA +\int_0^t \int_0^{s_1} A^2X(s_2) \dd s_2 \dd s_1
\\ &=& I + tA + \frac12(tA)^2 + \int_0^t \int_0^{s_1} \int_0^{s_3} A^3X(s_3) \dd s_2 \dd s_1
\end{IEEEeqnarray*}
We see the Taylor expansion of $\exp$ appear, as a generalisation to matrices. This motivate the following definition:
\begin{definition}
If it converges, we define the \emph{exponential} $e^A$ of a square matrix A and the underlying function $\exp$ as the infinite sum 
\[e^A = \sum_{n=0}^\infty \frac{1}{n!}A^n\]
\end{definition}
\begin{lemme} \label{lem:exp}
The exponential of a matrix always converge
\end{lemme}
\begin{proof}
We use $\|.\|$ for the operator norm on matrices, keeping in mind that all matricies norms are equivalent. Now we have by basic properties of this norm that 
\[ \sum_{n=0}^N \|\frac{1}{n!}A^n\| 
\leq \sum_{n=0}^N \frac{1}{n!}\|A^n\| 
\leq \sum_{n=0}^N \frac{1}{n!}\|A\|^n \]

and this is the Taylor finite expansion of $\|A\|$, which converge for all value of $\|A\|$. As a result the sum is absolutely convergent for the operator norm, and then must converge.
\end{proof}
\begin{theoreme}
For a matrix $A$ and a scalar $t$, the quantity $e^{tA}$ is differentiable with respect to $t$ and has derivative $\dd/{\dd t} (e^{tA}) = Ae^{tA}$
\end{theoreme}
\begin{proof}
Each coordinate of the series is actually a convergent Taylor series in t and is then analytic. The theory of analytic functions tell us that they are $C^\infty$ and that we can derive term by term. However, we explain the argument of the proof in our case: As seen in \prettyref{lem:exp}, the sum converge absolutely, and in our case it converge locally uniformly in $t$ when $N\to\infty$, because if $|t|\leq t_{max}$, 
\[ \sum_{n=0}^N \|\frac{1}{n!}t^nA^n\|
\leq \sum_{n=0}^N \frac{1}{n!}(t_{max}\|A\|)^n\] converge without independence in $t$ for the last quantity. Similarily, \[ \frac{\dd}{\dd t}\sum_{n=0}^N\frac{1}{n!}t^nA^n
= \sum_{n=1}^N \frac{1}{(n-1)!}t^{n-1}A^n
= A\sum_{n=0}^N \frac{1}{n!}(tA)^n\]
converge locally uniformly to $Ae^{tA}$ for the same reason. As a result, since the partial sums and the derivatives of the partial sums converge locally uniformly, the limit is differentiable with derivative the limit of the derivatives of the partial sums, namely $Ae^{tA}$ and we have the result.
\end{proof}
All of this tell us that as we expected, $e^{tA}$ is a matrix solution to the linear differential equation.
\begin{corollaire}
The matrix $e^{tA}$ is a fundamental solution and each solution write $x(t)=e^{tA}x_0$
\end{corollaire}
\begin{proof}
We evaluate it in $t=0$ by direct calculation since the sum become finite: 
\[e^{0A}=\sum_{n=0}^\infty \frac{1}{n!}(0A)^n = I.\]
So the initial matrix is non singular, meaning that it will stay along the time non singular and $e^{tA}$ is a fundamental matrix solution with formula $x(t)=e^{tA}x_0$.
\end{proof}

For matrices that are not in a special form, we cannot explicitly compute the series of the exponential and then then neither the solutions. Specials forms of matrices whose exponential are commutable are for example the ones where the power of the matrix has a general formula, like diagonal matrices. We know that diagonal matrices are directly related to eigenvalues and eigenvectors. The eigenvector are the directions where the matrix act like in the one dimensional case and as often, it is a way to find similar results in the higher dimensional problems.

Let's investigate what happens in theses directions, and choose a candidate that look like a one dimensional solution,$e^{\l t}v$, for a real eigenvalue $\l$ of the matrix $A$ and the corresponding eigenvector~$\vv$. We compute its derivative and obtain 
\[\dd/{\dd t}(e^{\l t}v) =  e^{\l t}\l v = e^{\l t}Av = A(e^{\l t}v).\]
This give us indeed a non trivial solution since the eigenvector is non null. Theses kind of solutions can be seen in the computation of the exponential when we have a basis of eigenvectors and that in consequence, the matrix $A$ is diagonalisable like $A=PDP^{-1}$ where $D$ is the diagonal matrix of the eigenvalues and P is the non singular matrix formed by the eigenvector. Then 
\[ A^n= (PDP^{-1})^n = PDP^{-1}\cdots PDP^{-1} = PD^nP^{-1} \]
and 
\[ e^{tA} 
= Pe^{tD}P^{-1} 
= P\text{diag}(e^{\l_jt})P^{-1} 
= (v_1e^{\l_1t},\dots,v_ne^{\l_nt})P^{-1} 
\]
which is the fundamental matrix of the kind of solutions we find before, up to a reparametrization.

This was the simple case. More generally the matrix can have complex eigenvalues with complex eigenvectors, but the following lemma show us that complex solutions are made of real solutions :
\begin{lemme} \label{lem:complex}
If $\zz=\xx+i\yy$ is a complex solution of real and imaginary parts $\xx$ and $\yy$, then $\xx$ and $\yy$ are real solutions.
\end{lemme}
\begin{proof}
This can be shown by the simple fact that
\[\dotxx+i\dotyy =\dotzz = A\zz = A\xx+iA\yy \]
and that the real and respectively imaginary parts must be equals along the equalities, giving us $\dotxx=A\xx$ and $\dotyy=A\yy$ wich is the result.
\end{proof}
We must then consider complex solutions obtained by complex eigenvalues too. We sum up the results together in the following theorem.
\begin{theoreme} \label{th:eigensolutions}
For a complex eigenvalue $\s=\a+i\b$ of $A$ with eigenvector $w=u+iv$, we have the two independent solutions
\[e^{\a t}(\cos(\b t)u - \sin(\b t)v)\] 
\[e^{\a t}(\cos(\b t)v + \sin(\b t)u).\] 
In particular, if $\s$ is real, this give only one solution $e^{\a t}u.$ 
\end{theoreme}
\begin{proof}
First we have that $e^{\s t}w$ is a complex solution, since 
\[\ddt(e^{\s t}w) = e^{\s t}\s w = e^{\s t}Aw = A(e^{\s t}w).\]
Then by \prettyref{lem:complex}, the real and the imaginary parts are real solutions, we compute them by rewriting the complex solution:
\[e^{\s t}w = e^{\a t}(\cos(\b t) + i\sin(\b t))(u+iv)
= e^{\a t}(\cos(\b t)u - \sin(\b t)v) + ie^{\a t}(\cos(\b t)v + \sin(\b t)u).\]
We recognise the resulting solutions in the real and imaginary part and if we evaluate them in t=0, we get respectively $u$ and $v$. To be linearly dependent, $u$ should be proportional to $v$, i.e. $v=au$. But now $v$ cannot be null, and then $\b$ neither. As a result we can evaluate the solutions in $t=\pi/(2\b)$ and we obtain respectively $-e^{\a\pi/(2\b)}v$ and $e^{\a\pi/(2\b)}u$. These two points must be with the same proportion as in $t=0$. This is impossible as it would implies that $u=-av=-u=0$. The two solutions are then independents and the first part of the result is proven. It follow directly by setting $\b=0$ that the solutions is $e^{\a t}(\cos(0)u - \sin(0)v) = e^{\a t}u$ and  $e^{\a t}v$. But since the eigenvalue is real, the eigenvector is real too and $v=0$ letting only one non trivial solution.
\end{proof}
\begin{remarque}
Note that complex values come by pairs of conjugates, as well as the eigenvectors: \\
$Aw=\s w$ implies 
$$A\overline{w} = \overline{A}\overline{w} = \overline{Aw}= \overline{\s w} = \overline{\s}\overline{w}.$$
However, this doesn't give us a new solution because $$e^{\overline{\s} t}\overline{w}= \overline{e^{\s t}}\overline{w} = \overline{e^{\s t}w}$$
has the same real and imaginary part as $e^{\s t}w$ up to the sign. As a result, a complex eigenvalue give two solutions but together with its conjugate, they give us still two solutions, so we can find as many independent solutions as independent eigenvector we find, real or not.
\end{remarque}
\quad \\
Now we have to deal with the case when we don't have a basis of eigenvectors. In this case, some eigenvalue $\l$ with algebraic multiplicity $\m_a$ and a geometric multiplicity
$$\m_g=\dim\ker(A-\l I) < \m_a.$$
Such an eigenvalue is said \emph{defective}, and the matrix is said \emph{defective} too when it has at least one defective eigenvalue. We use here the concept of generalized eigenvector that come from the result about the Jordan form:

\begin{definition}
    A vector $\ww$ is a \emph{generalized eigenvector} of rank $m$  of a matrix A and corresponding to an eigenvalue $\l$, if it a vector (complex if $\l$ is complex) that satisfy
    $$(A-\l I)^m\ww=0 \quad\text{and}\quad (A-\l I)^{m-1}\ww\neq0$$
    for a $m\in\N^*$.
    
    A \emph{canonical} basis of generalised eigenvectors is a basis of generalised eigenvector such that for all generalized eigenvector $\ww$ of rank $m$ that is in the basis, for all $0<j<m$, $(A-\l I)^j\ww$ are generalised eigenvectors of rank $m-j$ with respect to $\l$, and are in the basis too.
\end{definition}

Note that with $m=1$, a generalised vector is a usual eigenvector.
    
\begin{theoreme}
    There always exist a canonical basis of generalised eigenvectors.
\end{theoreme}
\begin{proof}
    Without proof. See \com{some reference}.
\end{proof}

In term of computation, we start from $(A-\l I)\vv=0$ to find the eigenvectors. Then we search for some $\ww$ such that $(A-\l I)\ww=\vv$, assuring that $(A-\l I)^2\ww=(A-\l I)\vv=0$ and hence that $\ww$ is a generalised vector of rank 2. We continue like this making a chain of generalised vectors. Even if we decide to take into account complex vectors

Now we show how this is actually useful, by looking at the solution starting at a generalised eigenvector $\ww$ of rank $m$ with respect to the defective eigenvalue $\l$.
\begin{theoreme} \label{th:solutiondegeneree}
 For a canonical basis $B$ of generalised eigenvectors, we have a set of independent complex solutions with form $e^{t\l}p_\ww(B)$, where $\l$ is the eigenvalue assosiated to a generalised eigenvector $\ww$ of rank $m$ in the basis, and $p_\ww$ is a polynomial in $B[t]$ of degree $m-1$.
\end{theoreme}
\begin{proof}
We see easily that the basic property of the exponential that changes sum into product is true for the commutative matrices with the condition that the matrices commute. The proof is the same as in the real case as we have already proven absolute convergence. 
We write $A= \l I + (A-\l I) $ and $\l I$ is diagonal hence commutative with all matrices. Taking $\ww$ from a canonical basis, we get
\begin{IEEEeqnarray}{rCl} \label{eq:solutioncomplexe}
e^{tA}\ww 
&=& e^{t\l I + t(A-\l I)}\ww 
=e^{t\l I} e^{t(A-\l I)}\ww 
= e^{t\l}\sum_{n=0}^\infty \frac{1}{n!}t^n(A-\l I)^n\ww
= e^{t\l}\sum_{n=0}^{m-1} \frac{1}{n!}t^n\ww_n 
\end{IEEEeqnarray}
where the $\ww_n=(A-\l I)^n\ww$ are other generalised vectors in the canonical basis. So have indeed a polynomial in $t$ with generalised eigenvectors coefficients. These are surely independent solutions for different choices of $\ww$ since the generalised eigenvectors are supposed independent and they are the initial values of these solutions, the proof is complete.
\end{proof}
As a result with this theorem we obtain a independent set of $d$ complex solutions, where $d$ is the dimension of the space. But we know that eigenvalues and eigenvectors come by pairs of conjugates. It is the same for generalised eigenvectors and eigenvalues:
$$(A-\l I)^m\ww=0 \quad\text{and}\quad (A-\l I)^{m-1}\ww\neq0$$
implies
$$(A-\overline{\l} I)^m\overline{\ww}=0 \quad\text{and}\quad (A-\overline{\l} I)^{m-1}\overline{\ww}\neq0$$
So we will get solutions like \prettyref{eq:solutioncomplexe} by pairs of conjugates. by subtracting them or summing them, we get two real new solutions that we will call \emph{degenerated}:
$$
    e^{tA}\ww + e^{tA}\overline{\ww} 
    = e^{tA}(\ww+\overline{\ww})
    = 2e^{tA}\Re(\ww), 
$$
$$
    e^{tA}\ww - e^{tA}\overline{\ww} 
    = e^{tA}(\ww-\overline{\ww})
    = 2e^{tA}\Im(\ww).
$$
\com{prouver que on garde des solutions indépendantes}
\begin{corollaire} \label{cor:formesolutionlineaire}
    All solutions to linear system have coordinates that are linear combinations of the following functions :
    \begin{itemize}
    \item $e^{\l t}$ 
    \item $e^{\a t}\cos(\b t)$ , $e^{\a t}\sin(\b t)$
    \item $t^je^{\l t}$ , $t^je^{\a t}\cos(\b t)$ , $t^je^{\a t}\sin(\b t)$ 
    \end{itemize}
    where $\l$ is a real eigenvalue, $\s=\a+i\b$ is an imaginary eigenvalue, $0\leq j< m_a$ is a natural number with $m_a$ the algebraic multiplicity of $\s$. Note that each point is a generalisation of the precedent.
\end{corollaire}

\section{Stability}
In other terms, the \prettyref{th:eigensolutions} tell us that each non null real eigenvalue gives direction(s) where the trajectories are straight and of exponential velocity, each null eigenvalue give direction(s) where trajectories are fixed, and each non real eigenvalue gives subspace(s) (not lines) where the trajectories are like ellipses that change of size exponentially. The \prettyref{th:solutiondegeneree} add other sort of solutions as polynomials resized by a exponential.

All these considerations are only on the special directions. Depending of the sign of the real part of the eigenvalues, theses specials solutions go very fast to $0$, stay in an orbit, diverge very fast to infinity values, or maybee follow a polynomial. This motivate us to see how these specials solutions act together, what is the asymptotically comportment of trajectories, how stable is the origin, and make a classification about all theses factors.
First of all we define concepts about stability, and asymptotic converge. For this we put our self in a more general cadre which will be useful later, with a $C^1$ function $\FF$ and the equation 
\[\dotxx = \FF(\xx)\]
The regularity of $\FF$ give a flow $\phi(\xx_0,t)$ which encapsulate all solutions:
\begin{IEEEeqnarray*}{rCl}
\dot{\phi}(\xx_0,t) &=& \FF(\phi(\xx_0,t)) \\
\phi(\xx_0,0) &=& \xx_0
\end{IEEEeqnarray*}
\begin{definition}
    A solution x is \emph{Lyanupov stable} ( or \emph{L-stable}, or simply \emph{stable}) if the flow is continuous in $\xx_0$ and uniformly in $t$. Namely, if for all $\e>0$ there exist a $\d>0$ such that $\|\zz-\xx(0)\|<\d$ implies that for all $t\geq0$, $\|\phi(\zz,t)-\xx(t)\|<\e$.
\end{definition}
\begin{definition}
    Two solutions $\xx$ and $\yy$ are $\omega$\emph{-attracted} to each other if $\lim_{t\to\infty}\|\yy(t)-\xx(t)\| = 0$. The resulting equivalence class is called the \emph{basis of attraction}. A solution $\xx$ is said \emph{$\omega$-attracting} or \emph{attracting} if there is a $\d>0$ such that $\|\zz_0-\xx_0\|<0$ implies that $\phi(\zz_0,t)$ is in the basin of attraction of $\xx$. A solution is said \emph{globally $\omega$-attracting} (or \emph{globally attracting}) on a set if this set is in the basin of attraction. We do not need to specify the set if it's the all solution space.
\end{definition}
\begin{definition}
    A solution is \emph{asymptotically stable} if it is L-stable and attracting. It is said \emph{globally asymptotically stable} if it is L-stable and globally attracting
\end{definition}
\begin{lemme}
    Attractivity and L-stability are not consequence of the other in any sense. 
\end{lemme}
\begin{proof}
Any solution of $\dotxx=0$ is L-stable but any of them are attracting. We can construct a flow on the plane where all solutions tends to $(1,1)$, but all solutions that start in $\R\times\R_+^*$, even in the neighbourhood of $(1,1)$, goes around (0,0) before. \com{présenter la construction}
\end{proof}
\begin{lemme}
    When $\FF$ is linear, i.e. $\dotxx=A\xx$, all solutions have the same L-stability and attractivity.
\end{lemme}
\begin{proof}
In the linear case, $\phi$ is linear in the first variable, indeed $\phi(\xx_0,t) = e^{tA}\xx_0 =: X(t)\xx_0$. Now for any solution $\xx$ and all initial point $\zz$,
\[ \|\phi(\zz,t)-\xx(t)\| = \|X(t)\zz-X(t)\xx(0)\| = \|X(t)(\zz-\xx_0)\| =\|\phi((\zz-\xx_0),t)-\phi(0,t)\|, \] meaning that L-stability and attractivity is entirely determined by the stability of the trivial solution $\phi(0,t)=0$.
\end{proof}
\begin{remarque}
    Therefore, we can speak of the L-stability and the attractivity of a linear system, meaning that it applies to all solutions, or none of them, and doing it by looking at the trivial solution 0. In this case the system is L-stable if and only if there exists a $\d>0$ such that $\|\xx_0\|<\d$ implies that for all $t\geq0$, $\|\phi(\xx_0,t)\|<\e$. The system is attracting if and only if there exists a $\d>0$ such that $\|\xx_0\|<\d$ implies that $\phi(\xx_0,t) \to 0$.
\end{remarque}
 
 \begin{theoreme} \label{th:stablecondition}
     The linear system is L-stable if and only if each of its solutions is bounded for positive times.
 \end{theoreme}
 \begin{proof}
  Suppose the system is L-stable, and for contradiction that a solution $\xx$ is not bounded. Let $\d>0$ be the distance given by the stability, such that all solutions that start with a norm smaller than $\d$ don't go away the unit ball. We can then define an other solution $\yy=\d\xx/\|\xx(0)\|$ and $\|\yy(0)\|=\d$, so $\yy$ wont go away the unit ball and is bounded. This contradict the fact that $\xx$ and $\yy$ are proportionals and all solutions must be bounded.
  
  Suppose now that all solutions are bounded. Then the columns of $X=e^{tA}$ the fundamental system are bounded, implying that actually the norm of X is bounded (all norm on finite dimensional spaces are equivalent). Now we get 
  \[ \|\phi(x_0,t)\|=\|X(t)x_0\| 
  \leq \|X(t)\| \|\xx_0\| 
  \leq \max_{t\geq0} \|X(t)\| \|\xx_0\| \]
  which is smaller than any positive $\e$ as soon as $\|\xx_0\|<\e/\max_{t\geq0} \|X(t)\|$ and give us the stability of the sytem.
 \end{proof}
\begin{theoreme}
    The linear system is globally asymptotically stable if and only if it is attracting.
\end{theoreme}
\begin{proof}
By definition, global asymptotic stability implies global attractivity and so in particular attractivity with any radius condition. In the other direction, if it is attracting with radius condition $\d$, such that when $\|\xx_0\|<\d$, $\|\phi(\xx_0,t)\|\to0$. Then any solution $\xx$ can be written $\xx=\|\xx(0)\|\yy/\d$ where $\yy=\d\xx/\|\xx(0)\|$ is a proportional solution that start with norm $\d\|\xx(0)\|/\|\xx(0)\|=\d$ and is small enough to converge to zero, implying that $\xx=\xx(0)\yy/\d$ converge to zero too. The system is globally attracting by arbitrarity of $\xx$. Now that all solutions are converging to zero, they are all bounded and by \prettyref{th:stablecondition}, we know that the system is actually stable. Both condition of stability and global attractivity are reunited, the system is globally asymptotically stable.
\end{proof}
\begin{theoreme}
    The linear system is 
    \begin{enumerate}
     \item L-stable if and only if all the eigenvalues of the matrix have non positive real parts, and all the one that are defective have negative real part.
    \item globally asymptotically stable if and only if all the eigenvalues of the matrix
    have negative real parts.
    \end{enumerate}
\end{theoreme}
\begin{proof} 
\quad\\
\begin{enumerate}
\item 
By \prettyref{th:stablecondition}, we just have to show that the conditions on the eigenvalues are equivalent to the fact that the solutions are bounded. The \prettyref{cor:formesolutionlineaire} told us the possible forms of all solutions. The functions $t^je^{\a t}\cos(\b t)$ , $t^je^{\a t}\sin(\b t)$ describe all the possibilities of linear combinations for the coordinates.

For both, if the eigenvalue $\s=\a+i\b$ is non defective, then $j=0$ and they are bounded if $\a$ is non positive. If $\s$ is defective, $j>0$ and they are bounded if $\a$ is negative because the exponential is $O(t^j)$ for all $j$'s. The sytem is now stable.

Alternatively, if there exists an eigenvalue $\s=\a+i\b$ with eigenvector $w=u+iv$ that have a positive real part, then by \prettyref{th:eigensolutions}, there exist a solution $e^{\a t}(cos(\b t)u-\sin(\b t)v)$ and its norm $e^{\a t}\|cos(\b t)u-\sin(\b t)v\|$
is not bounded since $cos(\b t)u-\sin(\b t)v$ doesn't converge to zero. If $\s$ is defective and is purely imaginary, then \prettyref{th:solutiondegeneree} tell us that there exist a solution $p_w(t)$, polynomial of non null degree, and $p_w(t)\to\infty$ when $t\to\infty$ because of the dominant term. There exists solutions that are unbounded , and the system is not stable.

\item 
By \prettyref{th:stablecondition}, we just have to show that the conditions on the eigenvalues are equivalent to the fact that the system is attracting.

But following the considerations of the first part, if all eigenvalues have negative real parts, there is always a $e^{\a t}$ with $\a<0$ term in front of the bases solutions and they all converge to zero. The system is attracting, and globally asymptotically stable.

Alternatively if it's not the case and that $\s=\a+i\b$ has a non negative part, we have a solution $e^{\a t}p_w(t)$ that repect $\|e^{\a t}p_w(t)\| \geq \|p_w(t)\|$ which doesn't go to zero. The system is not attracting and then not globally asymptotically stable.
\end{enumerate}
\end{proof}

This is the result we wanted in this section. It motivate the categorisation of linear systems, by taking to account the nature of their stability. We make the difference when there is a rotation effect or not
\begin{definition}
    For a sytem $\dotxx=A\xx$, we set the following denomination of the matrix $A$ and more generally of the system, regarding the nature of the eigenvalues of $A$.
    \begin{itemize}
    \item \emph{Stable} : all eigenvalues are real, negative and non defective. ($\R^*_+$)
    \com{revoir notation pour ne pas confondre stable et L-stable\dots}
    \item \emph{Source (unstable)} : $-A$ is stable. i.e. all eigenvalues are real, positive, and non defective. ($\R^*_-$)
    \item \emph{elliptic (center)} : all eigenvalues are non null, purely imaginary, and non defective. ($i\R^*$)
    \item \emph{stable focus (sink)} : all eigenvalues are non null, with negative real part, and non defective. ($\R^*_-+i\R$)
    \item \emph{saddle} : there exists a real and positive eigenvalue, and a real and negative one, both non defective.
    \item \emph{hyperbolic} : stable, source, or saddle. i.e. all eigenvalues are real, non null, and non defective.
    \item \emph{degenerated} : there exists a defective eigenvalue.
    \end{itemize}
\end{definition}
\com{graphiques et présentations des structures possibles des solutions}
\begin{corollaire}
\quad
\begin{itemize} 
    \item Stables, elliptics, and stables focus linear systems are L-stable.
    \item Sources, saddles linear systems are not L-stable.
    \item Stable and stable focus linear systems are globally asymptotically stable.
    \end{itemize} 
\end{corollaire}